{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T11:05:21.834372Z",
     "start_time": "2020-09-22T11:05:20.900023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_seq_items = 500\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import Counter\n",
    "import string\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    }
   },
   "source": [
    "# Named entity recognition with Spacy\n",
    "---\n",
    "\n",
    "- To improve search, named entities like people's names, organization and locations can be a useful enrichment of metadata. \n",
    "- I can imagine using entities like persons or organizations as explicit filter criteria for a search application. Or the NER could be part of an NLP pipeline and used to remove e.g., person's names as stop words.\n",
    "- [Spacy offers named entity recognition (NER) out of the box](https://spacy.io/usage/linguistic-features#named-entities) and provides three models for German language. \n",
    "- In this notebook, I try to assess how good and performant Spacy's entity recognition is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    }
   },
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6636, 29)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"_data/podcasts_cleaned.parq\")\n",
    "\n",
    "# Remove all podcast that weren't updated after 2018.\n",
    "df.releaseDate = pd.to_datetime(df.releaseDate)\n",
    "to_drop = df[df.releaseDate.dt.year<2019].index\n",
    "df.drop(to_drop, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll join texts in all relevant feature columns into one single text feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_text_columns(data):\n",
    "    text = []\n",
    "    for row in data.values:\n",
    "        if type(row) is str:\n",
    "            text.append(row)\n",
    "        elif isinstance(row, (np.ndarray, list)):\n",
    "            text.append(\" \".join(row))\n",
    "        else:\n",
    "            assert row is None\n",
    "            text.append(\"\")\n",
    "            continue\n",
    "    return \" \".join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_columns = ['artistName', 'title', 'subtitle', 'summary', \"tags\", \n",
    "                   'ep_titles', 'ep_itunes_titles', 'ep_tags', 'primary_genre',\n",
    "                   'ep_subtitles', 'ep_summaries', 'ep_contents']\n",
    "df_pod = df[podcast_columns].apply(join_text_columns, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_ASCII = re.compile(r\"[^A-Za-zÀ-žäüöÄÜÖ.!? ]\", re.IGNORECASE)\n",
    "MAX_WORDS = 10_000\n",
    "REMOVE_LESS_THAN = 2\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Reduce to MAX_WORDS to speed up processing.\n",
    "    text = \" \".join(text.split(\" \")[:MAX_WORDS])\n",
    "    \n",
    "    # Strip HTML tags.\n",
    "    text = BeautifulSoup(text, \"lxml\").get_text()\n",
    "    \n",
    "    # Keep only ASCII + European Chars, some punctuation and whitespace\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "   \n",
    "    # Remove links.\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'www.\\S+', ' ', text)\n",
    "    \n",
    "    # Remove all words less than 2 digits long.\n",
    "    text = \" \".join([token for token in text.split(\" \") if len(token)>REMOVE_LESS_THAN])\n",
    "       \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.4 s, sys: 1.79 s, total: 43.1 s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_pod = df_pod.apply(clean_text)\n",
    "df[\"text\"] = df_pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Detect entities\n",
    "\n",
    "In a first step I retrieve all entities in every text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b58f194b5a4a9998c1c310ca1b6cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47min 52s, sys: 18min 48s, total: 1h 6min 40s\n",
      "Wall time: 16min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ents_list = []\n",
    "\n",
    "# We have long texts in the data and need to increase Spacy's text length limit accordingly.\n",
    "nlp.max_length = 10_000_000\n",
    "\n",
    "# Use nlp.pipe with batches to speed up entity recognition. \n",
    "# Increasing n_process does not decrease processing times.\n",
    "for doc in tqdm(nlp.pipe(df.text.values, \n",
    "                             batch_size=50, \n",
    "                             n_process=1, \n",
    "                             disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])):\n",
    "        ents_list.append([(token.text, token.label_) for token in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save entity list to disk to avoid repeated time consuming runs of entity extraction.\n",
    "# with open(\"_data/entity_list.pkl\", \"wb+\") as file:\n",
    "#     pickle.dump(ents_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"_data/entity_list.pkl\", \"rb\") as file:\n",
    "    ents_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Spacy recognized around 316k person names, 76k locations, 143k organisations and 334k miscellaneous entities.**\n",
    "- By flattening the list, using a counter and printing the 50 most and least common entities, I can see that **the results are promising.**\n",
    "- However, I notice **a lot of errors** too. **Especially less common entities are noisy.**\n",
    "- We have to keep in mind, that many podcasts have hundreds of available episodes that we joined into the text document per podcast. In order to find common terms *across* podcasts we have to count differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Spacy recognized 316,925 unique entities of type PER.\n",
      "\n",
      "['Instagram', 'Link', 'Jesus', 'Corona', 'Jahren', 'Sebastian', 'Thomas', 'Chris', 'Daniel', 'Christian', 'Sohn', 'Alex', 'http', 'David', 'Trump', 'Michael', 'Andreas', 'Philipp', 'Anna', 'Jan', 'Sascha', 'Max', 'Martin', 'Donald Trump', 'bit.ly', 'Julian', 'Stefan', 'http bit.ly', 'linktr.ee', 'Tobi', 'Matthias', 'Tobias Teichen', 'Julia', 'Angela Merkel', 'Simon', 'Sven', 'Friedrich Nietzsche', 'Lena', 'Merkel', 'Markus', 'Hugo Egon Balder', 'Flo', 'Florian', 'Johannes', 'Felix', 'Tobias', 'Nina', 'Tim', 'Peter', 'Danke']\n",
      "\n",
      "['Thomas Sachsenmaier Simon Ihlenfeldt', 'Daniela Schröder leisure Was', 'Daniela Schröder Minding', 'konflikt mann', 'beziehungsdrama beziehungskonflikte dauersingle', 'Michaela Beck Free Your Heart Podcast', 'Tobias Maucher', 'Frank Eilers Sebastian Krämer Wie', 'You Can', 'Frank Eilers Sebastian Krämer', 'Peter Schanz Christian Eitner', 'Instagram Jackie Freitag Hauptprofil Instagram Jackie', 'Instagram Jackie Freitag Hauptprofil Instagram Jackie Freitag.de Coachingprofil', 'reconnect seelenwunde selbsthass', 'koerperliebe lauraseiler', 'Befreeiungsschläge', 'Jackie Freitag Reconnect Jackie Freitag', 'Corona Willkommensfolge', 'Lara Seelengespräche', 'Sonja Haar Lara Kammerer', 'Pekingönte', 'Meisterlingtac', 'Luca Köpping', 'Asgodom', 'Kobayashi Maru']\n",
      "\n",
      "________________________________________________________________________________\n",
      "Spacy recognized 76,603 unique entities of type LOC.\n",
      "\n",
      "['Deutschland', 'Berlin', 'Corona', 'USA', 'Kalifornien', 'Europa', 'Bayern', 'Deutschlands', 'Hamburg', 'Erde', 'München', 'China', 'Österreich', 'Stadt', 'Köln', 'Schweiz', 'Wien', 'NRW', 'Frankreich', 'Italien', 'Japan', 'Europas', 'Afghanistan', 'Russland', 'New York', 'Düsseldorf', 'Bremen', 'Paris', 'Frankfurt', 'Schweden', 'Spanien', 'Polen', 'Nürnberg', 'Australien', 'England', 'London', 'Stuttgart', 'Dortmund', 'Amerika', 'Indien', 'Infos', 'Leipzig', 'Afrika', 'Hause', 'Israel', 'Türkei', 'Essen', 'Jährige', 'Hessen', 'deutschland']\n",
      "\n",
      "['Busbahnhof. Oder was weis ich wohin. Kannst', 'Flughafen. Oder', 'Statistischen Bundesamt', 'Staatstheater Braunschweig April', 'Schloss braunschweig hip hop jazzkantine', 'Schloss Der Diamantenherzog', 'Schloss Das Hip Hop Musical', 'Land Deutschlands Verantwortung für Europa', 'Singapur.Hans staunte', 'Typhus', 'Gebirgsrand', 'Kiautschou Bucht', 'Ratsapotheke', 'Schoner winzig', 'Schoner', 'Gehlsdorf', 'Evert Co. Lagerstraße', 'Bucht von Kiautschou', 'Haus der Laden', 'Kiautschou.', 'Kiautschou', 'Xiamen', 'Qingdao.', 'Mobilitätspodcast Roboshuttle Flugtaxis', 'Bundesrepublik Die Zehnerjahre Terrorismus der Popkultur Terrorismus']\n",
      "\n",
      "________________________________________________________________________________\n",
      "Spacy recognized 143,452 unique entities of type ORG.\n",
      "\n",
      "['Instagram', 'Apple', 'Bundesliga', 'SPD', 'CDU', 'WDR', 'Corona', 'NFL', 'Social Media', 'HSV', 'Corona Pandemie', 'Kirche', 'ARD', 'Radio Hamburg News Show', 'Tesla', 'FDP', 'NBA', 'Infos', 'DDR', 'BVB', 'Borussia Dortmund', 'Microsoft', 'AfD', 'DFB', 'Amazon', 'Bundestag', 'Bundeswehr', 'DLRG', 'Schalke', 'ZDF', 'Disney', 'Deutschland', 'RTL', 'NDR', 'SWR', 'HIER', 'Sky', 'Spotify', 'CSU', 'Hertha', 'EZB', 'Partei', 'Nintendo', 'VfB', 'Taliban', 'SAP', 'BONUS', 'FOLGE', 'Werder Bremen', 'UFC']\n",
      "\n",
      "['Mona Zink Gesellschaft und Kultur', 'Minding Way Leisure Was', 'Übernimm JETZT', 'keynote speaker online business side business', 'Jazzkantine', 'Jazzkantine arts braunschweig hip hop jazzkantine musical performing arts revolution', 'Diamantenherzog', 'alltag kommunikation powerfrau Mentale Gesundheit Was', 'Dreißig education education self improvement self improvement', 'LinkedIn stephanieverch', 'deutschen Marine', 'Werftaufenthalt', 'Zehnjahres Contract', 'INITIATIVE ICH', 'INITIATIVE ICH.', 'Bristol Myers Squibb', 'ZEIT Stiftung. science Klimawandel', 'ZEIT Stiftung. Wissenspodcast', 'Vereinigung Deutscher Wissenschaftler Einer muss', 'Peanut Hour', 'brothers brüder business charakterspiel', 'sport learnings mindset', 'Bundeszentrale für politische Bildung.', 'Labels. film', 'Home Video']\n",
      "\n",
      "________________________________________________________________________________\n",
      "Spacy recognized 334,218 unique entities of type MISC.\n",
      "\n",
      "['Facebook', 'Zeit', 'YouTube', 'deutschen', 'Twitter', 'deutsche', 'Youtube', 'iTunes', 'twitter.com', 'Internet', 'Weihnachten', 'Pandemie', 'sozialen', 'Berliner', 'Corona Krise', 'Hört', 'Infos', 'Videos', 'Amazon', 'Google', 'Netflix', 'Spotify', 'Spiele', 'soziale', 'PODCAST', 'Lockdown', 'Bibel', 'DAX', 'Business', 'Hamburger', 'Deutschen', 'Deutsche', 'Send voice message anchor.fm', 'deutscher', 'WhatsApp', 'Instagram', 'Deutsch', 'Virus', 'Schweizer', 'ZEIT', 'Talk', 'TikTok', 'iOS', 'deutschsprachigen', 'Kölner', 'Bitcoin', 'europäischen', 'iPhone', 'Champions League', 'Euro']\n",
      "\n",
      "['DIE Frage Herzlich Willkommen', 'Schritt. Herzlich willkommen Free Your Heart Podcast. Herzlich willkommen', 'Mini Email Kurs', 'Und dein Herz', 'Der Glückskiller', 'Warum ziehst Dir', 'Schritt. Herzlich willkommen Free Your Heart Podcast. Ziehst Dir', 'Herzlich willkommen Free Your Heart Podcast. Und ein essenzieller Schlüssel', 'traummann traumpartner', 'partnerschaft partnersuche', 'Wer bin ich und warum bin ich hier?', 'self improvement single society culture', 'Lisa Tschirschke Phantom Crew', 'keynote speaker new work online business speaking', 'Der Diamantenherzog', 'Muttivation', 'Schön dass bist! bist', 'Wie Deine Seele mit Dir', 'Seelengespräche Für ein Leben Einklang mit Deiner Seele Was die Seele wirklich', 'Lara religion spirituality spirituality', 'Für ein Leben Einklang mit Deiner Seele', 'Comedy Herzlich Willkommen', 'Was das Land', 'Moin ihr Lieben', 'selbstfürsorge self improvement selfcare society culture']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ent_types = [\"PER\", \"LOC\", \"ORG\", \"MISC\"]\n",
    "for ent_type in ent_types:\n",
    "    cnt = Counter([y[0] for x in ents_list for y in x if y[1]==ent_type])\n",
    "    print(\"_\"*80)\n",
    "    print(f\"Spacy recognized {len(cnt):,.0f} unique entities of type {ent_type}.\\n\")\n",
    "    print([x[0] for x in cnt.most_common(50)])\n",
    "    print()\n",
    "    print([x[0] for x in cnt.most_common()[::-1][:25]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filter results\n",
    "To remove as much errors as possible I filter out all entities that do not occur in at least `n` unique podcasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate entities by type.\n",
    "results = []\n",
    "for row in ents_list:\n",
    "    per = [x[0] for x in row if x[1]==\"PER\"]\n",
    "    loc = [x[0] for x in row if x[1]==\"LOC\"]    \n",
    "    org = [x[0] for x in row if x[1]==\"ORG\"]\n",
    "    msc = [x[0] for x in row if x[1]==\"MISC\"] \n",
    "    results.append((per, loc, org, msc))\n",
    "    \n",
    "columns = [\"person\", \"location\", \"organisation\", \"misc\"]\n",
    "df_ents = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Set cutoff to filter out noisy entities.\n",
    "# An entity has to occur in min_occurence unique podcasts or more.\n",
    "min_occurence = 5\n",
    "\n",
    "ents_filtered = {}\n",
    "\n",
    "# Create a dictionary with entity types as keys and the corresponding list of filtered entities as the value. \n",
    "for column in columns:\n",
    "    cnt = Counter([y for x in df_ents[column].values for y in set(x)]).most_common()\n",
    "    ents_filtered[column] = [x[0] for x in cnt if x[1]>=min_occurence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13695829b356499d83bc4d299b99ff2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 77.2 ms, total: 1min 20s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Filter out all likely erroneous entities from the data set, row by row.\n",
    "for column in tqdm(columns):\n",
    "    df_ents[column] = df_ents[column].apply(lambda x: [y for y in x if y in ents_filtered[column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ents.to_parquet(\"_data/entity_list_cleaned.parq\")\n",
    "# df_ents = pd.read_parquet(\"_data/entity_list_cleaned.parq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persons\n",
    "Looking at unique terms and counting in how many podcasts these occur I notice:\n",
    "\n",
    "- `Corona` (very comprehensibly) ranks very high and occurs in the metadata of more than 1k podcasts. (It's misclassified as a person's name, but nonetheless...)\n",
    "- Politician's names rank high: `Angela Merkel`, `Donald Trump`, `Joe Biden` etc.\n",
    "- We get a lot of first names, which are less useful.\n",
    "- The names of several «usual suspects» like `James Bond`, `Elon Musk` rank high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('jahren', 1099), ('corona', 1026), ('link', 984), ('instagram', 884), ('sohn', 550), ('christian', 291), ('angela merkel', 290), ('thomas', 288), ('donald trump', 278), ('jesus', 267), ('michael', 267), ('trump', 262), ('daniel', 254), ('anna', 253), ('alex', 244), ('ehrlich', 234), ('julia', 217), ('andreas', 216), ('http', 214), ('sebastian', 212), ('teufel', 202), ('merkel', 200), ('david', 196), ('martin', 193), ('stefan', 190), ('philipp', 184), ('jan', 178), ('besser', 176), ('danke', 175), ('matthias', 172), ('markus', 164), ('elon musk', 160), ('joe biden', 157), ('peter', 151), ('max', 150), ('olaf scholz', 149), ('armin laschet', 143), ('chris', 142), ('dir', 139), ('james bond', 138), ('simon', 138), ('gott', 137), ('hass', 135), ('patrick', 135), ('felix', 134), ('covid', 132), ('christoph', 131), ('florian', 131), ('sich', 130), ('katharina', 126), ('johannes', 125), ('maria', 125), ('silvester', 125), ('vaters', 123), ('biden', 122), ('paul', 122), ('alexander', 122), ('gottes', 121), ('tobias', 120), ('sven', 120), ('jonas', 116), ('bit.ly', 116), ('julian', 115), ('dennis', 114), ('sarah', 113), ('jens spahn', 113), ('frank', 113), ('hitler', 110), ('homeoffice', 108), ('tobi', 106), ('hacks', 104), ('mann', 103), ('jens', 103), ('sascha', 101), ('tim', 100), ('annalena baerbock', 99), ('starke', 99), ('engel', 99), ('sandra', 98), ('sohnes', 98), ('nie', 97), ('marco', 94), ('lena', 93), ('nina', 93), ('marcel', 93), ('anne', 92), ('laura', 92), ('scholz', 90), ('greta thunberg', 89), ('marie', 89), ('http bit.ly', 89), ('tom', 88), ('apropos', 88), ('lisa', 88), ('stephan', 87), ('nico', 87), ('kevin', 86), ('moritz', 85), ('will', 85), ('lars', 85)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter([y.lower() for x in df_ents.person.apply(lambda x: set(x)).values for y in x ])\n",
    "print(cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we reduce the printout to just the entities that consists of two or more words, we get more meaningful results. \n",
    "- Still, I notice errors like `http bit.ly`, `instagram kanal` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angela merkel', 'donald trump', 'elon musk', 'joe biden', 'olaf scholz', 'armin laschet', 'james bond', 'jens spahn', 'annalena baerbock', 'greta thunberg', 'http bit.ly', 'bill gates', 'jeff bezos', 'george floyd', 'karl lauterbach', 'markus söder', 'steve jobs', 'tobias beck', 'christian lindner', 'dirk kreuter', 'gerald hüther', 'jesus christus', 'robert habeck', 'jogi löw', 'dieter bohlen', 'stefanie stahl', 'friedrich merz', 'kevin kühnert', 'laura malina seiler', 'frank thelen', 'helene fischer', 'albert einstein', 'thomas gottschalk', 'christian drosten', 'britney spears', 'sebastian kurz', 'barack obama', 'instagram kanal', 'udo lindenberg', 'günther jauch', 'boris johnson', 'uli hoeneß', 'herbert grönemeyer', 'til schweiger', 'sherlock holmes', 'hansi flick', 'gerhard schröder', 'warren buffett', 'veit lindau', 'sebastian fitzek', 'arnold schwarzenegger', 'micky beisenherz', 'jürgen klopp', 'luisa neubauer', 'joshua kimmich', 'kanye west', 'julien backhaus', 'tony robbins', 'felix lobrecht', 'joachim löw', 'markus lanz', 'christian bischoff', 'bob dylan', 'martin luther', 'julian reichelt', 'franziska giffey', 'horst seehofer', 'mark zuckerberg', 'thomas müller', 'jan böhmermann', 'tim mälzer', 'adolf hitler', 'oliver pocher', 'sven lorenz', 'maxim mankevich', 'michael jackson', 'eckart von hirschhausen', 'carolin kebekus', 'heidi klum', 'verena pausder', 'mareike awe', 'angela merkels', 'karl marx', 'sophie passmann', 'john strelecky', 'louisa dellert', 'kool savas', 'calvin hollywood', 'lothar matthäus', 'billie eilish', 'attila hildmann', 'laura seiler', 'julian nagelsmann', 'bruce springsteen', 'joe bidens', 'stephen king', 'indiana jones', 'philipp amthor', 'steven spielberg', 'robert lewandowski']\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter([y.lower() for x in df_ents.person.apply(lambda x: set(x)).values for y in x if len(y.split())>1])\n",
    "print([x[0] for x in cnt.most_common(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locations\n",
    "- Since we got the podcast data from the German iTunes store it's not suprising that `deutschland` ranks highest. The term occurs in the metadata of more than 2.7k podcasts. \n",
    "- A couple of errors are noticeable here too: e.g., `corona`, `veröffentlichung`, `jährig`, `beiden`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('deutschland', 2721), ('berlin', 1584), ('corona', 1416), ('usa', 1216), ('europa', 1073), ('deutschlands', 961), ('hamburg', 807), ('münchen', 738), ('erde', 721), ('stadt', 644), ('österreich', 618), ('china', 558), ('köln', 529), ('frankreich', 497), ('bayern', 486), ('schweiz', 483), ('italien', 452), ('europas', 432), ('russland', 395), ('wien', 385), ('new york', 380), ('paris', 353), ('japan', 343), ('london', 313), ('spanien', 309), ('indien', 304), ('england', 297), ('afrika', 290), ('australien', 289), ('stuttgart', 288), ('amerika', 287), ('schweden', 286), ('frankfurt', 279), ('düsseldorf', 279), ('leipzig', 267), ('afghanistan', 262), ('hause', 261), ('polen', 254), ('türkei', 252), ('israel', 250), ('essen', 244), ('großbritannien', 240), ('den usa', 226), ('nrw', 217), ('dortmund', 209), ('infos', 209), ('staaten', 199), ('bremen', 198), ('asien', 195), ('griechenland', 191), ('kanada', 190), ('kalifornien', 182), ('hollywood', 181), ('rom', 180), ('dresden', 175), ('portugal', 174), ('mond', 173), ('landes', 172), ('jährige', 171), ('alpen', 168), ('nürnberg', 168), ('dänemark', 167), ('norwegen', 166), ('brüssel', 162), ('amerikaner', 161), ('mallorca', 160), ('ungarn', 159), ('hannover', 159), ('brasilien', 156), ('ostsee', 150), ('südafrika', 144), ('iran', 144), ('veröffentlichung', 144), ('mainz', 143), ('mexiko', 138), ('ukraine', 137), ('mars', 137), ('brandenburg', 135), ('chinas', 134), ('woher', 133), ('home office', 132), ('syrien', 131), ('thailand', 130), ('zürich', 127), ('augsburg', 125), ('thüringen', 124), ('halle', 124), ('corona pandemie', 122), ('neuseeland', 122), ('ruhrgebiet', 121), ('kiel', 120), ('beiden', 119), ('südamerika', 119), ('republik', 119), ('niedersachsen', 118), ('ägypten', 114), ('der schweiz', 114), ('hessen', 114), ('belgien', 113), ('berlins', 112)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter([y.lower() for x in df_ents.location.apply(lambda x: set(x)).values for y in x])\n",
    "print(cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting only multi word terms in recognized locations reveals more meaningful entities, but more errors too: `home office`, `instagram kanal`, `youtube kanal` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new york', 380), ('den usa', 226), ('home office', 132), ('corona pandemie', 122), ('der schweiz', 114), ('los angeles', 94), ('silicon valley', 93), ('baden württemberg', 87), ('rheinland pfalz', 76), ('vereinigten staaten', 71), ('st. pauli', 63), ('bundesrepublik deutschland', 61), ('san francisco', 59), ('instagram kanal', 57), ('youtube kanal', 55), ('las vegas', 49), ('new yorker', 48), ('new york city', 48), ('frankfurt main', 48), ('höhle der löwen', 47), ('tel aviv', 44), ('sachsen anhalt', 42), ('schleswig holstein', 38), ('nordrhein westfalen', 37), ('sri lanka', 33), ('mecklenburg vorpommern', 31), ('corona welle', 28), ('costa rica', 28), ('bayern münchen', 27), ('new orleans', 26), ('wall street', 25), ('mount everest', 24), ('berliner mauer', 22), ('new work', 22), ('kölner dom', 21), ('erzbistum köln', 21), ('hong kong', 21), ('saudi arabien', 18), ('weiße haus', 18), ('sand meer', 17), ('corona lockdowns', 17), ('gran canaria', 17), ('st. petersburg', 17), ('tik tok', 17), ('dach region', 16), ('schwäbischen alb', 15), ('stadt köln', 15), ('auswärtigen amt', 15), ('rhein main', 15), ('prenzlauer berg', 15), ('corona zeit', 15), ('rocky mountains', 14), ('new yorks', 14), ('washington d.c.', 14), ('buenos aires', 14), ('vereinigten königreich', 13), ('vereinigten staaten von amerika', 13), ('bund und ländern', 13), ('starnberger see', 13), ('smart city', 13), ('hambacher forst', 13), ('lüneburger heide', 13), ('griechischen inseln', 13), ('europäischen ländern', 12), ('den niederlanden', 12), ('brandenburger tor', 12), ('bahnhof zoo', 12), ('stadt hamburg', 12), ('stadt land', 12), ('britische regierung', 11), ('jurassic park', 11), ('bayerischen wald', 11), ('flughafen ber', 11), ('die schweiz', 11), ('allianz arena', 11), ('auswärtige amt', 11), ('san diego', 10), ('stadt berlin', 10), ('die niederlande', 10), ('mülheim der ruhr', 10), ('stadt münchen', 10), ('frankfurter flughafen', 10), ('west berlin', 10), ('schweizer alpen', 10), ('sommerhaus der stars', 10), ('universität bremen', 10), ('rio janeiro', 10), ('vereinigten arabischen emiraten', 9), ('tagebau garzweiler', 9), ('kapitol washington', 9), ('côte azur', 9), ('burkina faso', 9), ('nrw landtag', 9), ('home schooling', 9), ('grand canyon', 9), ('world trade center', 8), ('baker street', 8), ('ampel regierung', 8), ('abu dhabi', 8), ('wembley stadion', 8)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter([y.lower() for x in df_ents.location.apply(lambda x: set(x)).values for y in x if len(y.split())>1])\n",
    "print(cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizations\n",
    "- `Corona` again ranks very high and again is misclassified.\n",
    "- I notice a lot of meaningful terms that actually are organisations.\n",
    "- There are a lot of errors too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('corona', 680), ('instagram', 652), ('corona pandemie', 571), ('social media', 509), ('apple', 444), ('spd', 435), ('cdu', 388), ('deutschland', 384), ('infos', 384), ('kirche', 361), ('bundesliga', 346), ('ard', 341), ('ddr', 332), ('bundestag', 314), ('fdp', 303), ('zdf', 287), ('spiegel', 275), ('wdr', 259), ('afd', 249), ('amazon', 248), ('dfb', 222), ('bundeswehr', 218), ('microsoft', 211), ('partei', 202), ('ndr', 188), ('hört', 176), ('die grünen', 175), ('disney', 172), ('grüne', 171), ('rtl', 167), ('tesla', 164), ('grünen', 162), ('csu', 156), ('zentrum', 155), ('spotify', 140), ('schalke', 133), ('bvb', 123), ('who', 121), ('linkedin', 120), ('fifa', 119), ('pandemie', 118), ('hsv', 117), ('gehst', 117), ('hier', 114), ('swr', 113), ('bmw', 112), ('adac', 112), ('taliban', 111), ('gemeinschaft', 111), ('porsche', 108), ('wir', 107), ('union', 103), ('bayern münchen', 103), ('nato', 101), ('ikea', 100), ('nationalmannschaft', 96), ('sony', 95), ('westdeutscher rundfunk', 95), ('alles', 95), ('the', 92), ('daimler', 92), ('leipzig', 91), ('ebay', 91), ('lufthansa', 91), ('china', 90), ('faz', 87), ('werder bremen', 87), ('eu', 87), ('sap', 87), ('berlin', 84), ('sky', 83), ('telekom', 83), ('borussia dortmund', 83), ('ezb', 81), ('sowjetunion', 80), ('uefa', 80), ('folge', 80), ('köln', 79), ('audi', 77), ('katholischen kirche', 75), ('volkswagen', 75), ('bundeskanzlerin', 73), ('nasa', 72), ('kommission', 72), ('nfl', 72), ('warum', 72), ('europa', 71), ('deutsche bahn', 70), ('nintendo', 70), ('demokraten', 70), ('freiburg', 70), ('bayern', 70), ('eure', 69), ('wirecard', 68), ('hertha', 68), ('charité', 67), ('europäischen union', 67), ('vfb stuttgart', 66), ('astrazeneca', 66), ('dax', 66)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter([y.lower() for x in df_ents.organisation.apply(lambda x: set(x)).values for y in x])\n",
    "print(cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('corona pandemie', 571), ('social media', 509), ('die grünen', 175), ('bayern münchen', 103), ('westdeutscher rundfunk', 95), ('werder bremen', 87), ('borussia dortmund', 83), ('katholischen kirche', 75), ('deutsche bahn', 70), ('europäischen union', 67), ('vfb stuttgart', 66), ('hertha bsc', 63), ('europäische union', 62), ('deutschen bahn', 61), ('deutschen nationalmannschaft', 61), ('new york times', 60), ('union berlin', 60), ('katholische kirche', 59), ('eintracht frankfurt', 57), ('deutschen bundestag', 55), ('start ups', 52), ('new work', 50), ('deutschland österreich', 49), ('frankfurter buchmesse', 48), ('deutsche nationalmannschaft', 48), ('bayerischer rundfunk', 48), ('vereinten nationen', 47), ('weimarer republik', 47), ('corona krise', 45), ('fridays for future', 45), ('vfl wolfsburg', 45), ('covid pandemie', 44), ('wir uns', 44), ('bündnis die grünen', 43), ('borussia mönchengladbach', 42), ('bayer leverkusen', 42), ('mehr infos', 41), ('free agency', 41), ('saudi arabien', 40), ('deutsche bank', 39), ('sozialen medien', 39), ('gesellschaft und kultur', 39), ('black lives matter', 38), ('st. pauli', 38), ('die linke', 35), ('bundeszentrale für politische bildung', 33), ('vfl bochum', 33), ('deutschen telekom', 33), ('die digitalisierung', 31), ('manchester united', 31), ('instagram facebook', 31), ('universität hamburg', 30), ('humboldt universität berlin', 30), ('holstein kiel', 30), ('red bull', 30), ('den grünen', 29), ('lmu münchen', 29), ('manchester city', 29), ('athletic greens', 29), ('fortuna düsseldorf', 29), ('arminia bielefeld', 29), ('deutscher meister', 29), ('robert koch institut', 28), ('die welt', 28), ('real madrid', 28), ('deutschen bank', 27), ('bayerischen rundfunk', 27), ('sars cov', 27), ('deutsche telekom', 25), ('rolling stones', 25), ('die bahn', 25), ('hass netz', 25), ('frankfurter allgemeine zeitung', 25), ('black friday', 25), ('mecklenburg vorpommern', 25), ('black lives matter bewegung', 25), ('schleswig holstein', 25), ('universität köln', 25), ('universität wien', 25), ('spd grünen', 24), ('fake news', 24), ('die afd', 24), ('rocket beans', 24), ('capital bra', 24), ('cancel culture', 24), ('coca cola', 23), ('hört rein!', 23), ('tsg hoffenheim', 22), ('fsv mainz', 22), ('rock roll', 21), ('ndr info', 21), ('corona hilfen', 21), ('pen paper', 21), ('charité berlin', 21), ('... wir', 21), ('viva con agua', 21), ('dynamo dresden', 21), ('leipziger buchmesse', 20), ('der spiegel', 20), ('new york', 20)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter([y.lower() for x in df_ents.organisation.apply(lambda x: set(x)).values for y in x if len(y.split())>1])\n",
    "print(cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous entities\n",
    "- As to be expected we get a wide variety of terms for entity type `MISC`.\n",
    "- We get a lot of terms with pronouns, as well as fragments of sentences: `wir freuen uns`, `viel spaß`, `send voice message anchor.fm` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('zeit', 3563), ('deutschen', 2263), ('deutsche', 2005), ('youtube', 1871), ('facebook', 1723), ('weihnachten', 1273), ('internet', 1105), ('sozialen', 805), ('pandemie', 778), ('twitter', 747), ('berliner', 739), ('deutscher', 731), ('corona krise', 690), ('hört', 679), ('videos', 650), ('soziale', 649), ('itunes', 528), ('lockdown', 511), ('infos', 491), ('netflix', 466), ('hamburger', 458), ('virus', 439), ('amazon', 433), ('schweizer', 419), ('amerikanischen', 398), ('google', 388), ('europäischen', 380), ('podcast', 377), ('millionen', 374), ('anschluss', 373), ('deutschsprachigen', 365), ('euro', 359), ('twitter.com', 350), ('spiele', 342), ('business', 340), ('deutsch', 333), ('amerikanische', 324), ('bundestagswahl', 317), ('whatsapp', 303), ('talk', 287), ('spotify', 287), ('instagram', 278), ('europäische', 269), ('krieg', 265), ('ostern', 257), ('englischen', 254), ('corona', 247), ('kölner', 247), ('quarantäne', 242), ('bibel', 238), ('einfach', 233), ('englisch', 229), ('tiktok', 229), ('tages', 227), ('münchner', 224), ('wikipedia', 209), ('französische', 208), ('coronakrise', 208), ('französischen', 206), ('weihnachtszeit', 200), ('chinesische', 197), ('englische', 193), ('chinesischen', 188), ('christentum', 184), ('apps', 184), ('das jahr', 175), ('britische', 174), ('wiener', 173), ('österreichischen', 173), ('juden', 171), ('österreichische', 171), ('japanische', 170), ('deutsches', 166), ('europameisterschaft', 166), ('sozialer', 165), ('italienische', 164), ('karlsruhe', 164), ('russischen', 163), ('star wars', 163), ('halloween', 163), ('herzlich willkommen', 159), ('iphone', 157), ('das leben', 155), ('britischen', 155), ('corona virus', 153), ('daniel', 152), ('corona zeit', 151), ('viel spaß', 151), ('viren', 149), ('der podcast', 149), ('vorfreude', 148), ('italienischen', 147), ('champions league', 147), ('frankfurter', 147), ('tinder', 146), ('tatort', 146), ('deutschland', 145), ('russische', 145), ('games', 144), ('weihnachtsfest', 141)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter([y.lower() for x in df_ents.misc.apply(lambda x: set(x)).values for y in x])\n",
    "print(cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('corona krise', 690), ('das jahr', 175), ('star wars', 163), ('herzlich willkommen', 159), ('das leben', 155), ('corona virus', 153), ('corona zeit', 151), ('viel spaß', 151), ('der podcast', 149), ('champions league', 147), ('die zeit', 123), ('spotify open.spotify.com', 103), ('dieses jahr', 100), ('zweiten weltkrieg', 96), ('game thrones', 87), ('fake news', 85), ('special guest', 81), ('höchste zeit', 79), ('home office', 77), ('die zukunft', 76), ('facebook instagram', 76), ('new work', 76), ('apple podcast', 75), ('mein name', 72), ('send voice message anchor.fm', 69), ('ein interview', 68), ('olympischen spiele', 67), ('die wahrheit', 67), ('die angst', 66), ('youtube videos', 66), ('super league', 65), ('star trek', 63), ('olympischen spielen', 62), ('fridays for future', 61), ('youtube video', 61), ('feier des tages', 61), ('hör dir', 60), ('wir freuen uns', 59), ('der weg', 56), ('freut euch', 54), ('true crime podcast', 53), ('frohe weihnachten', 52), ('corona lockdown', 52), ('die höhle der löwen', 52), ('youtube kanal', 52), ('super bowl', 52), ('twitter twitter.com', 52), ('lange zeit', 51), ('der mensch', 50), ('ein podcast', 49), ('europa league', 48), ('süddeutschen zeitung', 47), ('zweiten weltkriegs', 46), ('corona warn app', 46), ('neuen jahr', 46), ('ein gespräch', 45), ('harry potter', 45), ('deep dive', 45), ('mehr infos', 45), ('die welt', 44), ('gesetz der anziehung', 44), ('neue jahr', 44), ('die sache', 44), ('corona welle', 43), ('das neue jahr', 43), ('premier league', 43), ('tag für tag', 43), ('instagram facebook', 43), ('wenn dir', 43), ('squid game', 42), ('deutsche sprache', 42), ('itunes podcasts.apple.com', 42), ('apple podcasts', 42), ('die geschichte', 41), ('das buch', 41), ('social media', 41), ('zeit für uns', 40), ('win win', 39), ('corona jahr', 39), ('burn out', 39), ('mein podcast', 39), ('tour france', 38), ('dfb pokal', 37), ('diesem jahr', 37), ('freu dich', 37), ('die bibel', 36), ('kalten krieg', 36), ('die liebe', 36), ('olympische spiele', 36), ('resident evil', 36), ('apple watch', 35), ('virtual reality', 35), ('true crime', 35), ('wie dir', 35), ('deep talk', 35), ('happy end', 34), ('fest der liebe', 34), ('spotify open.spotify.com show', 34), ('xbox series', 33), ('facebook twitter', 33)]\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter([y.lower() for x in df_ents.misc.apply(lambda x: set(x)).values for y in x if len(y.split())>1])\n",
    "print(cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "- I assess the results as promising. \n",
    "- The detected entities in many cases are correct and meaningful. \n",
    "- There are a lot of errors and misclassifications too. The resulting collections of terms need further cleaning, e.g. by removing stop words. \n",
    "- I assume that a «perfect» cleaning isn't feasible with reasonable effort. The variety of errors seems too wide. Nonetheless, the entities could prove very useful.\n",
    "- The detection is fast – processing the whole data set just took 15 minutes (Mac mini M1)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
