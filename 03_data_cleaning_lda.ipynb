{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T11:05:21.834372Z",
     "start_time": "2020-09-22T11:05:20.900023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "params = {\n",
    "    'text.color': (0.25, 0.25, 0.25),\n",
    "    'figure.figsize': [18, 6],\n",
    "   }\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_seq_items = 500\n",
    "\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "np.random.seed(42)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import string\n",
    "import glob\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "import xmltodict\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import scipy.spatial.distance\n",
    "\n",
    "import umap\n",
    "\n",
    "TITLE_SIZE = 24\n",
    "TITLE_PAD = 20\n",
    "\n",
    "DEFAULT_COLORS = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T11:05:21.834372Z",
     "start_time": "2020-09-22T11:05:20.900023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T11:05:21.834372Z",
     "start_time": "2020-09-22T11:05:20.900023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ----------- NLP modules ----------- #\n",
    "\n",
    "import langdetect\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases, hdpmodel, LdaModel, CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "# Data cleaning for LDA\n",
    "---\n",
    "In this notebook I prepare the pre-cleaned data for modelling with LDA.\n",
    "\n",
    "- [X] Concatenate content\n",
    "- [X] Create a domain specific extended stop word list\n",
    "- [X] Detect texts in languages other than German\n",
    "- [X] Strip HTML, links, remove non-ASCII characters, remove stop words etc.\n",
    "- [X] Remove names\n",
    "- [X] Lemmatize\n",
    "- [X] Tokenize\n",
    "- [X] Extract n-grams\n",
    "\n",
    "I extend the exploratory insights by retrieving more statistics from the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"_data/podcasts_cleaned.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6636, 29)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all podcast that weren't updated after 2018.\n",
    "df.releaseDate = pd.to_datetime(df.releaseDate)\n",
    "to_drop = df[df.releaseDate.dt.year<2019].index\n",
    "df.drop(to_drop, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "## Concatenate textual content\n",
    "\n",
    "I concatenate the available textual content to two single features – one for the texts about the podcast and one for the texts that describe the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def join_text_columns(data):\n",
    "    text = []\n",
    "    for row in data.values:\n",
    "        if type(row) is str:\n",
    "            text.append(row)\n",
    "        elif isinstance(row, (np.ndarray, list)):\n",
    "            text.append(\" \".join(row))\n",
    "        else:\n",
    "            assert row is None\n",
    "            text.append(\"\")\n",
    "            continue\n",
    "    return \" \".join(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "podcast_columns = ['artistName', 'title', 'subtitle', 'summary']\n",
    "\n",
    "episode_columns = ['ep_authors', 'ep_titles', 'ep_itunes_titles', \n",
    "                   'ep_subtitles', 'ep_summaries', 'ep_contents']\n",
    "\n",
    "df_pod = df[podcast_columns].apply(join_text_columns, axis=1) \n",
    "df_eps = df[episode_columns].apply(join_text_columns, axis=1) \n",
    "\n",
    "# Sanity check.\n",
    "assert df_pod.shape[0] == df_eps.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "Sanity check with single samples. Have we properly joined all columns and textual content? It seems we have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idx = 1703\n",
    "# display(df_pod[idx][:1000])\n",
    "# print()\n",
    "# display(df[podcast_columns].loc[idx].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and extend stop word list\n",
    "\n",
    "I'll use Spacy's stop word list. However, I extend this list with many additional stop words, that I found by analysing the most frequent non-salient words from all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"_data/_input/stopwords.txt\") as file:\n",
    "    stopwords = file.readlines()\n",
    "stopwords = sorted(set([x.strip() for x in stopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "## Detect foreign language samples\n",
    "\n",
    "I already have filtered out all podcasts that weren't tagged as German. Nonetheless I validate this by running a language detection on the concatenated texts that describe the podcast (title, subtitle, summary).\n",
    "\n",
    "- **I find ~200 podcasts for which German is not the detected predominant language.**\n",
    "- 140 samples are detected as English. A couple of other languages like Dutch, Portuguese and Swedish were detected too.\n",
    "- **Only a handful of the detected samples are actually not German language podcasts. I decide to leave these in the data for now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "non_german = []\n",
    "for idx, text in tqdm(enumerate(df_pod.values)):\n",
    "    try:\n",
    "        language = langdetect.detect_langs(text)\n",
    "        if str(language[0]).split(\":\")[0]!=\"de\":\n",
    "            non_german.append((idx, language))\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lang = pd.DataFrame(non_german, columns=[\"idx\", \"language\"])\n",
    "df_lang.to_csv(\"_data/language_detection.csv\")\n",
    "print(f\"{len(df_lang)} podcasts found that might contain mostly non German textual content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "display(df_lang.language.apply(lambda x: x[0].lang).value_counts()[:top_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df.loc[df_lang.idx.values][[\"artistName\", \"title\", \"subtitle\", \"summary\", \"primary_genre\", \"not_longtail\"]].head(top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "## Clean texts\n",
    "\n",
    "In a first cleaning step I do this:\n",
    "\n",
    "- [X] strip HTML\n",
    "- [X] keep only ASCII and european characters \n",
    "- [X] remove hyperlinks\n",
    "- [X] remove words shorter than 2 characters\n",
    "- [X] remove stop words (based on the extended stop word list)\n",
    "\n",
    "Since the lemmatization in the next step only works properly on cased text, I'll lowercase after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = list(spacy.lang.de.stop_words.STOP_WORDS)\n",
    "STOP_WORDS.extend(sorted(list(stopwords)))\n",
    "STOP_WORDS = set(STOP_WORDS)\n",
    "\n",
    "RE_ASCII = re.compile(r\"[^A-Za-zÀ-žäüöÄÜÖ ]\", re.IGNORECASE)\n",
    "\n",
    "REMOVE_LESS_THAN = 2\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Strip HTML tags.\n",
    "    text = BeautifulSoup(text, \"lxml\").get_text()\n",
    "    \n",
    "    # Normalize form of unicode strings.\n",
    "    # https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize\n",
    "    # text = unicodedata.normalize(\"NFKD\", text)\n",
    "    \n",
    "    # Keep only ASCII + European Chars and whitespace\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "\n",
    "    # # Lower case all text.\n",
    "    # text = text.lower()\n",
    "    \n",
    "    # Remove links.\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    \n",
    "    # Remove all words less than 2 characters long.\n",
    "    text = \" \".join([token for token in text.split(\" \") if len(token)>REMOVE_LESS_THAN])\n",
    "\n",
    "    # # Remove consecutive hyphens (long or short ones).\n",
    "    # text = re.sub(r\"[-–]+\", \"-\", text)\n",
    "    \n",
    "    # Remove stop words.\n",
    "    text = \" \".join([token for token in text.split(\" \") if token.lower() not in STOP_WORDS])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pod_clean = df_pod.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# I use multiprocessing for this CPU bound job of speeding up text cleaning.\n",
    "# multiprocessing.Pool unfortunately doesn't work in iPython. \n",
    "# Therefore I use «multiprocess» which is a package that has to be installed separately.\n",
    "# https://stackoverflow.com/a/65001152/7117003\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    df_eps_clean = pool.map(clean_text, df_eps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pod_clean = pd.DataFrame(df_pod_clean, columns=[\"pod_text\"])\n",
    "df_eps_clean = pd.DataFrame(df_eps_clean, columns=[\"eps_text\"])\n",
    "\n",
    "df_pod_clean.to_parquet(\"_data/clean_pod.parq\")\n",
    "df_eps_clean.to_parquet(\"_data/clean_eps.parq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "## Remove names\n",
    "\n",
    "- From first tests and iterations with LDA I could see that peoples names rank high as topic terms. A person's name in most of the cases I examined was noise, e.g., the creators name is not significant with respect to a topic and does not help the model to generalize to unseen samples. \n",
    "- However, names like `Angela Merkel`, `Joe Biden` or `Pablo Picasso` would be salient information in regard to a topic, so I have to make a tradeoff here. \n",
    "- I decide to filter out names from the data because most of the names do not seem salient in regard to the task of finding meaningful topic vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pod_clean = pd.read_parquet(\"_data/clean_pod.parq\")\n",
    "df_eps_clean = pd.read_parquet(\"_data/clean_eps.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shorten episode texts to 10_000 characters.\n",
    "df_eps_clean_short = pd.DataFrame(df_eps_clean.eps_text.apply(lambda x: x[:10_000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499ec56e0846422d9e4e251890216b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3a6007de79498a94e370693b45466c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 57s, sys: 5min 32s, total: 18min 30s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "names_list = []\n",
    "\n",
    "# To accommodate for long texts I increase Spacy's text length limit.\n",
    "nlp.max_length = 10_000_000\n",
    "\n",
    "# Use nlp.pipe with batches to speed up entity recognition. \n",
    "# Note: Increasing n_process does not decrease processing times.\n",
    "for data in [df_pod_clean.pod_text.values, df_eps_clean_short.eps_text.values]:\n",
    "    for doc in tqdm(nlp.pipe(data, \n",
    "                             batch_size=50, \n",
    "                             n_process=1, \n",
    "                             disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])):\n",
    "        names_list.extend([token.text for token in doc.ents if token.label_==\"PER\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "We can see that the entity recognition makes errors. E.g. a topic term like `Corona` gets labeled as a person. Also `Jesus`, `Sohn`, `Mann`, `Hass` are salient topical terms rather than names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"_data/_input/stopwords_names_raw.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Corona', 1261),\n",
       " ('Jesus', 1056),\n",
       " ('Sohn', 358),\n",
       " ('Angela Merkel', 305),\n",
       " ('Donald Trump', 278),\n",
       " ('Bobby Schuller', 276),\n",
       " ('Trump', 260),\n",
       " ('Mann', 241),\n",
       " ('Tobias', 234),\n",
       " ('Merkel', 216),\n",
       " ('Hass', 210),\n",
       " ('Sebastian', 205),\n",
       " ('Rainer Zitelmann', 192),\n",
       " ('Olaf Scholz', 179),\n",
       " ('Joe Biden', 174),\n",
       " ('Thomas', 172),\n",
       " ('Anna', 169),\n",
       " ('Dirk Kreuter', 159),\n",
       " ('Armin Laschet', 158),\n",
       " ('Teufel', 155)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt = Counter(names_list)\n",
    "display(cnt.most_common(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names list reduced to 1263 terms.\n"
     ]
    }
   ],
   "source": [
    "# Reduce to most common names of n_min.\n",
    "n_min = 20\n",
    "names_list = [x[0] for x in cnt.most_common() if x[1]>=n_min]\n",
    "\n",
    "# Sort stop word list with names so that first we replace the longer terms and then the shorter ones.\n",
    "stop_words_names = sorted(names_list, key=lambda x: len(x), reverse=True)\n",
    "print(f\"Names list reduced to {len(stop_words_names)} terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use replace instead of regex. \n",
    "# This is ~2x faster but less flexible.\n",
    "def clean_names(data):\n",
    "    for name in stop_words_names:\n",
    "        data = data.replace(f\" {name} \", \" \")\n",
    "    return re.sub(r\"\\s+\", \" \", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 s, sys: 9.44 ms, total: 2.17 s\n",
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_pod_clean.pod_text = df_pod_clean.pod_text.apply(clean_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411bdcad27454a5294afc66f1bc24d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 27s, sys: 806 ms, total: 4min 27s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmp = []\n",
    "for text in tqdm(df_eps_clean.eps_text.values):\n",
    "    tmp.append(clean_names(text))\n",
    "\n",
    "df_eps_clean.eps_text = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pod_clean.to_parquet(\"_data/clean_pod_names.parq\")\n",
    "df_eps_clean.to_parquet(\"_data/clean_eps_names.parq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "## Lemmatize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 777 ms, sys: 64.2 ms, total: 841 ms\n",
      "Wall time: 843 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pod = []\n",
    "\n",
    "# Lemmatize episode texts.\n",
    "# Use nlp.pipe with batches to speed up lemmatization. \n",
    "# Increasing n_process didn't result in shorter processing times.\n",
    "for doc in nlp.pipe(df_pod_clean.pod_text.values, batch_size=50, n_process=1, disable=['tok2vec', 'tagger', 'morphologizer', 'parser', 'attribute_ruler', 'ner']):\n",
    "    results_pod.append(\" \".join([token.lemma_ for token in doc]))\n",
    "    \n",
    "results_pod = [x.lower() for x in results_pod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 954 ms, total: 2min 8s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_eps = []\n",
    "\n",
    "nlp.max_length = 10_000_000\n",
    "\n",
    "# Lemmatize podcasts texts.\n",
    "# Use nlp.pipe with batches to speed up lemmatization. \n",
    "# Increasing n_process didn't result in shorter processing times.\n",
    "for doc in nlp.pipe(df_eps_clean.eps_text.values, batch_size=50, n_process=1, disable=['tok2vec', 'tagger', 'morphologizer', 'parser', 'attribute_ruler', 'ner']):\n",
    "    results_eps.append(\" \".join([token.lemma_ for token in doc]))\n",
    "    \n",
    "results_eps = [x.lower() for x in results_eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_pod = pd.DataFrame(results_pod, columns=[\"pod_text\"])\n",
    "results_eps = pd.DataFrame(results_eps, columns=[\"eps_text\"])\n",
    "\n",
    "results_pod.to_parquet(\"_data/clean_pod_lemma.parq\")\n",
    "results_eps.to_parquet(\"_data/clean_eps_lemma.parq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "## Statistics of textual content after data cleaning\n",
    "- **95% of podcasts have 6 words or more describing text on podcast level and 60 words or more on episode level.**\n",
    "- In next steps I might remove samples with shorter texts (the *other* 5%) from the data set, assuming that these do not contain sufficient information for meaningful modelling.\n",
    "- **Interestingly, Podcasts from charts or top lists have more than 3 times the word count compared to longtail podcasts.** \n",
    "- The volume of text data that describes the episode of a podcast is ~164 times the volume of that, which describes the podcast itself (title, subtitle, summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5/95% percentiles are 6 / 88 words in podcast texts.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    6636\n",
       "mean       37\n",
       "std        29\n",
       "min         1\n",
       "25%        18\n",
       "50%        32\n",
       "75%        51\n",
       "max       759\n",
       "Name: pod_text, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = results_pod.pod_text.apply(lambda x: len(x.split(\" \")))\n",
    "all_pod = tmp.sum()\n",
    "\n",
    "perc_5 = np.percentile(tmp, 5)\n",
    "perc_95 = np.percentile(tmp, 95)\n",
    "\n",
    "print(f\"The 5/95% percentiles are {perc_5:,.0f} / {perc_95:,.0f} words in podcast texts.\\n\")\n",
    "display(tmp.describe().astype(int))\n",
    "\n",
    "# df.loc[tmp[tmp<perc_5].index].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5/95% percentiles are 60 / 24,305 words in episode texts.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count      6636\n",
       "mean       6233\n",
       "std       13937\n",
       "min           1\n",
       "25%         585\n",
       "50%        2027\n",
       "75%        6303\n",
       "max      318957\n",
       "Name: eps_text, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = results_eps.eps_text.apply(lambda x: len(x.split(\" \")))\n",
    "all_eps = tmp.sum()\n",
    "\n",
    "perc_5 = np.percentile(tmp, 5)\n",
    "perc_95 = np.percentile(tmp, 95)\n",
    "\n",
    "print(f\"The 5/95% percentiles are {perc_5:,.0f} / {perc_95:,.0f} words in episode texts.\\n\")\n",
    "display(tmp.describe().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podcasts from top lists or charts in this data set have 327% the textual content of longtail podcasts.\n",
      "Textual data for episodes is 164 times the text volume for podcasts.\n"
     ]
    }
   ],
   "source": [
    "df[\"text_word_count\"] = tmp.values\n",
    "df[\"top\"]= np.where(df.not_longtail>0, True, False)\n",
    "tmp_median = df.groupby(\"top\").text_word_count.median()\n",
    "ratio = 100 / tmp_median[False] * tmp_median[True]\n",
    "print(f\"Podcasts from top lists or charts in this data set have {ratio:.0f}% the textual content of longtail podcasts.\")\n",
    "\n",
    "all_words = all_pod + all_eps\n",
    "ratio = 100 / all_pod * all_eps\n",
    "print(f\"Textual data for episodes is {ratio/100:.0f} times the text volume for podcasts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "source": [
    "## Tokenize and extract n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906 unique bigrams found for podcast texts.\n",
      "CPU times: user 424 ms, sys: 9.5 ms, total: 434 ms\n",
      "Wall time: 432 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "pod_bigrams = []\n",
    "tmp = [x[0].split(\" \") for x  in results_pod.values]\n",
    "\n",
    "# min_count ==  Ignore all words and bigrams with total collected count lower than this value.\n",
    "# threshold == Threshold for forming the phrases (higher means fewer phrases).\n",
    "bigram = Phrases(tmp, min_count=5, threshold=10)\n",
    "\n",
    "for idx in range(len(tmp)):\n",
    "    for token in bigram[tmp[idx]]:\n",
    "        if '_' in token:\n",
    "            pod_bigrams.append(token)\n",
    "            tmp[idx].append(token)\n",
    "\n",
    "results_pod_tokens = tmp\n",
    "unique_pod_bigrams = sorted(set(pod_bigrams))        \n",
    "print(f\"{len(unique_pod_bigrams)} unique bigrams found for podcast texts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T13:09:41.740211Z",
     "start_time": "2020-09-22T13:09:41.734330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269417 unique bigrams found for episode texts.\n",
      "CPU times: user 1min 9s, sys: 2.77 s, total: 1min 12s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "eps_bigrams = []\n",
    "tmp = [x[0].split(\" \") for x  in results_eps.values]\n",
    "\n",
    "bigram = Phrases(tmp, min_count=5, threshold=10)\n",
    "\n",
    "for idx in range(len(tmp)):\n",
    "    for token in bigram[tmp[idx]]:\n",
    "        if '_' in token:\n",
    "            eps_bigrams.append(token)\n",
    "            tmp[idx].append(token)\n",
    "\n",
    "results_eps_tokens = tmp\n",
    "unique_eps_bigrams = sorted(set(eps_bigrams))        \n",
    "print(f\"{len(unique_eps_bigrams)} unique bigrams found for episode texts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pod_tokens = pd.DataFrame(pd.Series(results_pod_tokens), columns=[\"pod_text\"])\n",
    "results_eps_tokens = pd.DataFrame(pd.Series(results_eps_tokens), columns=[\"eps_text\"])\n",
    "\n",
    "results_pod_tokens.to_parquet(\"_data/clean_pod_token.parq\")\n",
    "results_eps_tokens.to_parquet(\"_data/clean_eps_token.parq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Again, the data proved to be quite diverse and full of noise (e.g., lots of HTML, links).\n",
    "- The amount of information I have for each podcast is very diverse. Some podcasts have almost no metadata to work with. Others have very high word counts, mainly due to lots of available episodes and their extensive descriptions.\n",
    "- From episodes we get ~164 times the textual content than from podcasts. This is an inbalance we have to keep in mind for all further steps.\n",
    "- Interestingly, podcasts from charts or top lists have on average three times the textual data than those from less popular, long-tail podcasts. If this has statistical significance, we can speculate if either creators of successful podcasts have the resources to generate proper metadata or if podcasts are more likely to be successful if they have proper metadata."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
